INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='mooc', batch_size=200, model_name='Liquid', gpu=0, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, structure_feat_dim=50, lamda=0.3, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=200, channel_embedding_dim=50, max_input_sequence_length=64, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='Adam', weight_decay=0.0, patience=20, val_ratio=0.15, test_ratio=0.15, num_runs=5, test_interval_epochs=10, negative_sample_strategy='random', load_best_configs=False, device='cuda:0', seed=0, save_model_name='Liquid_seed0')
INFO:root:model -> Sequential(
  (0): Liquid(
    (memory_encoder): Linear(in_features=172, out_features=172, bias=True)
    (edge_encoder): Linear(in_features=172, out_features=172, bias=True)
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=7145, memory_dim=172)
    (memory_updater): CFCMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=7145, memory_dim=172)
      (memory_updater): MoE(
        (mixer): Linear(in_features=788, out_features=616, bias=True)
        (relu): ReLU(inplace=True)
        (experts): ModuleList(
          (0-2): 3 x rnnmodule(
            (cell): GRUCell(616, 172)
            (relu): ReLU()
          )
          (3-5): 3 x cfcmodule(
            (cell): cfcbundle(
              (rnn_cell): CfCCell(
                (backbone): Sequential(
                  (0): Linear(in_features=788, out_features=128, bias=True)
                  (1): LeCun(
                    (tanh): Tanh()
                  )
                )
                (tanh): Tanh()
                (sigmoid): Sigmoid()
                (ff1): Linear(in_features=128, out_features=172, bias=True)
                (ff2): Linear(in_features=128, out_features=172, bias=True)
                (time_a): Linear(in_features=128, out_features=172, bias=True)
                (time_b): Linear(in_features=128, out_features=172, bias=True)
              )
              (leaky_factor): Linear(in_features=616, out_features=172, bias=True)
            )
            (relu): ReLU()
          )
        )
        (softplus): Softplus(beta=1.0, threshold=20.0)
        (softmax): Softmax(dim=1)
      )
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0-1): 2 x MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0-1): 2 x MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
      (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
        (neighbor_co_occurrence_encode_layer): Sequential(
          (0): Linear(in_features=1, out_features=172, bias=True)
          (1): ReLU()
          (2): Linear(in_features=172, out_features=172, bias=True)
        )
      )
      (structure_encoder): Linear(in_features=172, out_features=172, bias=True)
      (relu): ReLU()
    )
    (neighbor_co_occurrence_encoder): NeighborCooccurrenceEncoder(
      (neighbor_co_occurrence_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=172, bias=True)
        (1): ReLU()
        (2): Linear(in_features=172, out_features=172, bias=True)
      )
    )
    (structure_encoder): Linear(in_features=172, out_features=172, bias=True)
    (relu): ReLU()
    (layer_norm_feat): LayerNorm((344,), eps=1e-05, elementwise_affine=True)
    (layer_norm_structure): LayerNorm((344,), eps=1e-05, elementwise_affine=True)
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: Liquid, #parameters: 15349956 B, 14990.19140625 KB, 14.638858795166016 MB.
The dataset has 411749 interactions, involving 7144 different nodes
The training dataset has 227485 interactions, involving 6015 different nodes
The validation dataset has 61762 interactions, involving 2599 different nodes
The test dataset has 61763 interactions, involving 2412 different nodes
The new node validation dataset has 25592 interactions, involving 2333 different nodes
The new node test dataset has 29179 interactions, involving 2181 different nodes
714 nodes were used for the inductive testing, i.e. are never seen during training
Traceback (most recent call last):
  File "/home/lhw/DyGLib/DyGLib/train_link_prediction.py", line 268, in <module>
    model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_neg_src_node_ids,
  File "/home/lhw/DyGLib/DyGLib/models/Liquid.py", line 146, in compute_src_dst_node_temporal_embeddings
    node_embeddings=self.layer_norm_feat(node_embeddings)
  File "/home/lhw/miniconda3/envs/lnntest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lhw/miniconda3/envs/lnntest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lhw/miniconda3/envs/lnntest/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 202, in forward
    return F.layer_norm(
  File "/home/lhw/miniconda3/envs/lnntest/lib/python3.10/site-packages/torch/nn/functional.py", line 2576, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Given normalized_shape=[344], expected input with shape [*, 344], but got input of size[400, 172]
